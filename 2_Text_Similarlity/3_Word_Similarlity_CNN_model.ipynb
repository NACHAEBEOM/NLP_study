{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_IN_PATH = './data/'\n",
    "DATA_OUT_PATH = './output/'\n",
    "\n",
    "TRAIN_Q1_DATA_FILE = 'train_q1.npy' # 기준이 되는 문장\n",
    "TRAIN_Q2_DATA_FILE = 'train_q2.npy' # 기준 문장과 비교할 대상이 되는 문장\n",
    "TRAIN_LABEL_DATA_FILE = 'train_label.npy' # 단어사전\n",
    "DATA_CONFIGS = 'data_configs.json' # 단어 사전의 크기값을 가지고 있는 파일\n",
    "\n",
    "TEST_SPLIT = 0.1\n",
    "RNG_SEED = 13371447"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH=1\n",
    "BATCH_SIZE=1024\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 31\n",
    "\n",
    "WORD_EMBEDDING_DIM = 100\n",
    "CONV_FEATURE_DIM = 300\n",
    "CONV_OUTPUT_DIM = 128\n",
    "CONV_WINDOW_SIZE = 3\n",
    "SIMILARITY_DENSE_FEATURE_DIM = 200\n",
    "\n",
    "prepro_configs = None\n",
    "\n",
    "with open(DATA_IN_PATH + DATA_CONFIGS, 'r') as f:\n",
    "    prepro_configs = json.load(f)\n",
    "    \n",
    "VOCAB_SIZE = prepro_configs['vocab_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_data = np.load(open(DATA_IN_PATH + TRAIN_Q1_DATA_FILE, 'rb'))\n",
    "q2_data = np.load(open(DATA_IN_PATH + TRAIN_Q2_DATA_FILE, 'rb'))\n",
    "labels = np.load(open(DATA_IN_PATH + TRAIN_LABEL_DATA_FILE, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  70, 1047,   47, ...,    0,    0,    0],\n",
       "       [  57,    4,   76, ...,    0,    0,    0],\n",
       "       [  16,   60,   18, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [   4,   21,    7, ...,    0,    0,    0],\n",
       "       [   2,   21, 7735, ...,    0,    0,    0],\n",
       "       [   9,   15,  302, ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  16,    3, 3394, ...,    0,    0,    0],\n",
       "       [   4,    9,   15, ...,    0,    0,    0],\n",
       "       [   2,   36,    1, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [   4,   11,  135, ...,    0,    0,    0],\n",
       "       [   2,   21, 7735, ...,    0,    0,    0],\n",
       "       [   3,   19,  240, ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q2_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.stack((q1_data, q2_data), axis=1)\n",
    "y = labels\n",
    "train_X, eval_X, train_y, eval_y = train_test_split(X, y, test_size=TEST_SPLIT, random_state=RNG_SEED)\n",
    "\n",
    "train_Q1 = train_X[:,0]\n",
    "train_Q2 = train_X[:,1]\n",
    "eval_Q1 = eval_X[:,0]\n",
    "eval_Q2 = eval_X[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 입력 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map 함수\n",
    "def rearrange(base, hypothesis, label):         # rearrange(기준 질문, 비교 대상 질문, 라벨값)\n",
    "    features = {\"x1\": base, \"x2\": hypothesis}   # 2개의 질문을 하나의 딕셔너리 형태의 입력값으로 만든다.\n",
    "    return features, label                      # 딕셔너리와 라벨을 리턴하는 구조 \n",
    "\n",
    "# 학습 입력 함수\n",
    "def train_input_fn():\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((train_Q1, train_Q2, train_y))\n",
    "    dataset = dataset.shuffle(buffer_size=10000)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.map(rearrange)\n",
    "    dataset = dataset.repeat(EPOCH)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    \n",
    "    return iterator.get_next()\n",
    "\n",
    "# 검증 입력 함수\n",
    "def eval_input_fn():\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((eval_Q1, eval_Q2, eval_y))\n",
    "    dataset = dataset.shuffle(buffer_size=10000)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.map(rearrange)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    \n",
    "    return iterator.get_next()\n",
    "\n",
    "\n",
    "## 두 입력 함수의 차이점은 검증 입력 함수는 EPOCH만큼 반복하지 않는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN 블록 함수 정의 * 합성곱 신경망과 풀링, DENSE를 하나로 합친 형태로 정의\n",
    "\n",
    "def basic_conv_sementic_network(inputs, name):\n",
    "    conv_layer = tf.keras.layers.Conv1D(CONV_FEATURE_DIM, \n",
    "                                        CONV_WINDOW_SIZE, \n",
    "                                        activation=tf.nn.relu, \n",
    "                                        name=name + 'conv_1d',\n",
    "                                        padding='same')(inputs)\n",
    "\n",
    "    max_pool_layer = tf.keras.layers.MaxPool1D(MAX_SEQUENCE_LENGTH, \n",
    "                                               1)(conv_layer)\n",
    "\n",
    "    output_layer = tf.keras.layers.Dense(CONV_OUTPUT_DIM, \n",
    "                                         activation=tf.nn.relu,\n",
    "                                         name=name + 'dense')(max_pool_layer)\n",
    "    output_layer = tf.squeeze(output_layer, 1)\n",
    "    \n",
    "    return output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(features, labels, mode):\n",
    "    TRAIN = mode == tf.estimator.ModeKeys.TRAIN\n",
    "    EVAL = mode == tf.estimator.ModeKeys.EVAL\n",
    "    PREDICT = mode == tf.estimator.ModeKeys.PREDICT\n",
    "    \n",
    "    embedding = tf.keras.layers.Embedding(VOCAB_SIZE,\n",
    "                                          WORD_EMBEDDING_DIM)\n",
    "    \n",
    "    base_embedded_matrix = embedding(features['x1'])\n",
    "    hypothesis_embedded_matrix = embedding(features['x2'])\n",
    "    \n",
    "    base_embedded_matrix = tf.keras.layers.Dropout(0.2)(base_embedded_matrix)\n",
    "    hypothesis_embedded_matrix = tf.keras.layers.Dropout(0.2)(hypothesis_embedded_matrix)  \n",
    "    \n",
    "    base_sementic_matrix = basic_conv_sementic_network(base_embedded_matrix, 'base')\n",
    "    hypothesis_sementic_matrix = basic_conv_sementic_network(hypothesis_embedded_matrix, 'hypothesis')  \n",
    "    \n",
    "    merged_matrix = tf.concat([base_sementic_matrix, hypothesis_sementic_matrix], -1)\n",
    "\n",
    "    similarity_dense_layer = tf.keras.layers.Dense(SIMILARITY_DENSE_FEATURE_DIM,\n",
    "                                             activation=tf.nn.relu)(merged_matrix)\n",
    "    # 유사도 측정을 위해 두 벡터에 대한 코사인 유사도 점수나 유클리디안 거리 점수를 활용할 수 있다.\n",
    "    \n",
    "    similarity_dense_layer = tf.keras.layers.Dropout(0.2)(similarity_dense_layer)    \n",
    "    logit_layer = tf.keras.layers.Dense(1)(similarity_dense_layer)\n",
    "    logit_layer = tf.squeeze(logit_layer, 1)\n",
    "    similarity = tf.nn.sigmoid(logit_layer)\n",
    "    \n",
    "    if PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "                  mode=mode,\n",
    "                  predictions={\n",
    "                      'is_duplicate':similarity\n",
    "                  })\n",
    "    # 예측값으로 측정한 유사도 값을 딕셔너리 형태로 전달\n",
    "    \n",
    "    loss = tf.losses.sigmoid_cross_entropy(labels, logit_layer)\n",
    "    # 손실값 계산\n",
    "\n",
    "    if EVAL:\n",
    "        accuracy = tf.metrics.accuracy(labels, tf.round(similarity))\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "                  mode=mode,\n",
    "                  eval_metric_ops= {'acc': accuracy},\n",
    "                  loss=loss)\n",
    "    # 정확도 값을 리턴하기 위해 값을 계산\n",
    "    # 측정한 유사도 값에 round 함수를 사용해 반올림하면 0혹인 1 값을 가지는데 이 값과 라벨을 비교해서 정확도를 측정\n",
    "    # 이렇게 측정한 정확도와 손실값을 리턴\n",
    "    \n",
    "    if TRAIN:\n",
    "        global_step = tf.train.get_global_step()\n",
    "        train_op = tf.train.AdamOptimizer(1e-3).minimize(loss, global_step)\n",
    "\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "                  mode=mode,\n",
    "                  train_op=train_op,\n",
    "                  loss=loss)\n",
    "    # 학습 상태인 경우의 함수를 리턴\n",
    "    # 학습 상태에는 단순히 손실값과 정확도를 측정하는 것보다 모델을 학습시켜서 가중치를 최적화해야 하기 때문에\n",
    "    # tf.train.AdamOptimizer를 생성 후 손실값을 적용해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'C:\\\\Python\\\\Python36\\\\venv\\\\kmu\\\\Scripts\\\\nlp\\\\Text_Similarlity\\\\./data/checkpoint/cnn/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001C18E994C18>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"6\" #For TEST GPU\n",
    "\n",
    "model_dir = os.path.join(os.getcwd(), DATA_IN_PATH + \"checkpoint/cnn/\")\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "est = tf.estimator.Estimator(model_fn, model_dir=model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into C:\\Python\\Python36\\venv\\kmu\\Scripts\\nlp\\Text_Similarlity\\./data/checkpoint/cnn/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.69342566, step = 1\n",
      "INFO:tensorflow:global_step/sec: 1.19153\n",
      "INFO:tensorflow:loss = 0.54236335, step = 101 (83.939 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.30482\n",
      "INFO:tensorflow:loss = 0.48890194, step = 201 (76.611 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 263 into C:\\Python\\Python36\\venv\\kmu\\Scripts\\nlp\\Text_Similarlity\\./data/checkpoint/cnn/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.46422404.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.estimator.Estimator at 0x1c18e994eb8>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est.train(train_input_fn) #train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-06-29-18:55:35\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Python\\Python36\\venv\\kmu\\Scripts\\nlp\\Text_Similarlity\\./data/checkpoint/cnn/model.ckpt-263\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-06-29-18:55:41\n",
      "INFO:tensorflow:Saving dict for global step 263: acc = 0.75429606, global_step = 263, loss = 0.49808544\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 263: C:\\Python\\Python36\\venv\\kmu\\Scripts\\nlp\\Text_Similarlity\\./data/checkpoint/cnn/model.ckpt-263\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'acc': 0.75429606, 'loss': 0.49808544, 'global_step': 263}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est.evaluate(eval_input_fn) #eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_Q1_DATA_FILE = 'test_q1.npy'\n",
    "TEST_Q2_DATA_FILE = 'test_q2.npy'\n",
    "TEST_ID_DATA_FILE = 'test_id.npy'\n",
    "\n",
    "test_q1_data = np.load(open(DATA_IN_PATH + TEST_Q1_DATA_FILE, 'rb'))\n",
    "test_q2_data = np.load(open(DATA_IN_PATH + TEST_Q2_DATA_FILE, 'rb'))\n",
    "test_id_data = np.load(open(DATA_IN_PATH + TEST_ID_DATA_FILE, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\python\\python36\\venv\\kmu\\lib\\site-packages\\tensorflow\\python\\estimator\\inputs\\queues\\feeding_queue_runner.py:62: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From c:\\python\\python36\\venv\\kmu\\lib\\site-packages\\tensorflow\\python\\estimator\\inputs\\queues\\feeding_functions.py:500: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Python\\Python36\\venv\\kmu\\Scripts\\nlp\\Text_Similarlity\\./data/checkpoint/cnn/model.ckpt-263\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "WARNING:tensorflow:From c:\\python\\python36\\venv\\kmu\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py:804: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n"
     ]
    }
   ],
   "source": [
    "predict_input_fn = tf.estimator.inputs.numpy_input_fn(x={\"x1\":test_q1_data,\n",
    "                                                         \"x2\":test_q2_data},\n",
    "                                                      shuffle=False)\n",
    "\n",
    "predictions = np.array([p['is_duplicate'] for p in est.predict(input_fn=predict_input_fn)])\n",
    "\n",
    "output = pd.DataFrame( data={\"test_id\":test_id_data, \"is_duplicate\": list(predictions)} )\n",
    "output.to_csv(\"./output/cnn_predict.csv\", index=False, quoting=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 캐글에서 0.52047으로 2572등(private) 정도 나온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
