{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_IN_PATH = './data/'\n",
    "DATA_OUT_PATH = './output/'\n",
    "TRAIN_CLEAN_DATA = 'train_clean.csv'\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "TEST_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(DATA_IN_PATH + TRAIN_CLEAN_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = list(train_data['review'])\n",
    "sentiments = list(train_data['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for review in reviews:\n",
    "    sentences.append(review.split())\n",
    "# word2vec을 사용하기 위해서는 입력 값을 단어로 구분된 리스트로 만들어야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\n",
    "   level=logging.INFO)\n",
    "# word2vec 학습 과정에서 로그 메세지를 양식에 맞게 INFO 수준으로 보여주는 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec 학습 시 필요한 하이퍼 파라미터\n",
    "num_features = 300 # 각 단어에 대해 임베딩된 벡터의 차원을 정한다\n",
    "min_word_count = 40 # 모델에 의미 있는 단어를 가지고 학습하기 위해 적은 빈도 수의 단어들은 학습하지 않는다.\n",
    "num_workers = 4 # 모델 학습 시 학습을 위한 프로세스 개수를 지정\n",
    "context = 10 # 컨텍스트 윈도우 크기\n",
    "downsampling = 1e-3 # 학습을 수행할 때 빠른 학습을 위해 정답 단어 라벨에 대한 다움샘플링 비율을 지정, 보통 0.001이 좋은 성능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 21:52:22,840 : INFO : 'pattern' package not found; tag filters are not available for English\n",
      "2019-06-27 21:52:22,861 : INFO : collecting all words and their counts\n",
      "2019-06-27 21:52:22,861 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-06-27 21:52:23,194 : INFO : PROGRESS: at sentence #10000, processed 1205223 words, keeping 51374 word types\n",
      "2019-06-27 21:52:23,473 : INFO : PROGRESS: at sentence #20000, processed 2396605 words, keeping 67660 word types\n",
      "2019-06-27 21:52:23,608 : INFO : collected 74065 word types from a corpus of 2988089 raw words and 25000 sentences\n",
      "2019-06-27 21:52:23,608 : INFO : Loading a fresh vocabulary\n",
      "2019-06-27 21:52:23,847 : INFO : effective_min_count=40 retains 8160 unique words (11% of original 74065, drops 65905)\n",
      "2019-06-27 21:52:23,847 : INFO : effective_min_count=40 leaves 2627273 word corpus (87% of original 2988089, drops 360816)\n",
      "2019-06-27 21:52:23,882 : INFO : deleting the raw counts dictionary of 74065 items\n",
      "2019-06-27 21:52:23,898 : INFO : sample=0.001 downsamples 30 most-common words\n",
      "2019-06-27 21:52:23,899 : INFO : downsampling leaves estimated 2494384 word corpus (94.9% of prior 2627273)\n",
      "2019-06-27 21:52:23,927 : INFO : estimated required memory for 8160 words and 300 dimensions: 23664000 bytes\n",
      "2019-06-27 21:52:23,927 : INFO : resetting layer weights\n",
      "2019-06-27 21:52:24,062 : INFO : training model with 4 workers on 8160 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2019-06-27 21:52:25,087 : INFO : EPOCH 1 - PROGRESS: at 18.94% examples, 477427 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-27 21:52:26,087 : INFO : EPOCH 1 - PROGRESS: at 45.44% examples, 566910 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-27 21:52:27,090 : INFO : EPOCH 1 - PROGRESS: at 71.94% examples, 598283 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-27 21:52:28,092 : INFO : EPOCH 1 - PROGRESS: at 98.48% examples, 611343 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-27 21:52:28,108 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-06-27 21:52:28,108 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-27 21:52:28,123 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-27 21:52:28,140 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-27 21:52:28,140 : INFO : EPOCH - 1 : training on 2988089 raw words (2494520 effective words) took 4.1s, 614524 effective words/s\n",
      "2019-06-27 21:52:29,144 : INFO : EPOCH 2 - PROGRESS: at 22.16% examples, 561540 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-27 21:52:30,162 : INFO : EPOCH 2 - PROGRESS: at 49.68% examples, 619808 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-27 21:52:31,171 : INFO : EPOCH 2 - PROGRESS: at 78.35% examples, 647354 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-27 21:52:31,908 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-06-27 21:52:31,920 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-27 21:52:31,928 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-27 21:52:31,940 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-27 21:52:31,940 : INFO : EPOCH - 2 : training on 2988089 raw words (2494525 effective words) took 3.8s, 657667 effective words/s\n",
      "2019-06-27 21:52:32,952 : INFO : EPOCH 3 - PROGRESS: at 16.94% examples, 427726 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-27 21:52:33,965 : INFO : EPOCH 3 - PROGRESS: at 43.41% examples, 541852 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-27 21:52:34,971 : INFO : EPOCH 3 - PROGRESS: at 68.70% examples, 570307 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-27 21:52:35,972 : INFO : EPOCH 3 - PROGRESS: at 89.32% examples, 554347 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-27 21:52:36,407 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-06-27 21:52:36,425 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-27 21:52:36,433 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-27 21:52:36,449 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-27 21:52:36,453 : INFO : EPOCH - 3 : training on 2988089 raw words (2494887 effective words) took 4.5s, 554030 effective words/s\n",
      "2019-06-27 21:52:37,467 : INFO : EPOCH 4 - PROGRESS: at 26.91% examples, 667574 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-27 21:52:38,489 : INFO : EPOCH 4 - PROGRESS: at 57.92% examples, 717088 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-27 21:52:39,479 : INFO : EPOCH 4 - PROGRESS: at 90.03% examples, 743018 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-27 21:52:39,821 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-06-27 21:52:39,839 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-27 21:52:39,847 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-27 21:52:39,855 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-27 21:52:39,859 : INFO : EPOCH - 4 : training on 2988089 raw words (2493941 effective words) took 3.4s, 734250 effective words/s\n",
      "2019-06-27 21:52:40,864 : INFO : EPOCH 5 - PROGRESS: at 27.52% examples, 686374 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-27 21:52:41,866 : INFO : EPOCH 5 - PROGRESS: at 56.52% examples, 707609 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-27 21:52:42,870 : INFO : EPOCH 5 - PROGRESS: at 80.79% examples, 668189 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-27 21:52:43,604 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-06-27 21:52:43,604 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-27 21:52:43,623 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-27 21:52:43,631 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-27 21:52:43,631 : INFO : EPOCH - 5 : training on 2988089 raw words (2494582 effective words) took 3.8s, 661384 effective words/s\n",
      "2019-06-27 21:52:43,631 : INFO : training on a 14940445 raw words (12472455 effective words) took 19.6s, 637423 effective words/s\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec # !pip install gensim\n",
    "\n",
    "model = word2vec.Word2Vec(sentences, workers=num_workers,\n",
    "           size=num_features, min_count = min_word_count,\n",
    "            window = context, sample = downsampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 21:55:41,573 : INFO : saving Word2Vec object under 300features_40minwords_10context, separately None\n",
      "2019-06-27 21:55:41,573 : INFO : not storing attribute vectors_norm\n",
      "2019-06-27 21:55:41,589 : INFO : not storing attribute cum_table\n",
      "c:\\python\\python36\\venv\\kmu\\lib\\site-packages\\smart_open\\smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
      "2019-06-27 21:55:41,875 : INFO : saved 300features_40minwords_10context\n"
     ]
    }
   ],
   "source": [
    "model_name = '300features_40minwords_10context'\n",
    "model.save(model_name)\n",
    "# 모델을 저장하는 코드, Word2Vec.load()를 통해 다시 사용 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LogisticRegression\n",
    "1. 학습을 하기 위해서 하나의 리뷰를 같은 형태의 입력값으로 만들어야 한다\n",
    "2. 위에서 처리한 데이터는 각 단어가 벡터로 표현되어 있다.\n",
    "3. 리뷰마다 단어의 개수가 다르기 때문에 입력값을 하나의 형태로 만들어야 한다.\n",
    "4. 가장 단순한 방법으로 문장에 있는 모든 단어의 벡터값에 대해 평균을 내서 리뷰 하나당 하나의 벡터로 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하나의 리뷰에 대해 전체 단어의 평균값을 계산하는 함수\n",
    "def get_features(words, model, num_features):\n",
    "    feature_vector = np.zeros((num_features),dtype=np.float32) # 출력 벡터 초기화\n",
    "\n",
    "    num_words = 0\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    # 어휘 사전 준비\n",
    "    \n",
    "    for w in words:\n",
    "        if w in index2word_set:\n",
    "            num_words += 1\n",
    "            feature_vector = np.add(feature_vector, model[w])\n",
    "            # 사전에 해당하는 단어에 대해 단어 벡터를 더함\n",
    "\n",
    "    feature_vector = np.divide(feature_vector, num_words)\n",
    "    # 문장의 단어 수만큼 나누어 단어 벡터의 평균값을 문장 벡터로\n",
    "    return feature_vector\n",
    "\n",
    "# get_features(words: 리뷰, model: 학습한 word2vec모델, num_features: word2vec으로 임베팅할 때 정한 벡터의 차원 수)\n",
    "# 하나의 벡터를 만드는 과정에서 속도를 빠르게 하기 위해 np.zeros를 사용해 0만 가지는 벡터를 만든다\n",
    "# 문장의 단어가 해당 모델 단어사전에 속하는지 보기 위해 model.wv.index2word를 set 객체로 생성\n",
    "# for문에서 반복문을 통해 리뷰를 구성하는 단어에 대해 임베딩된 벡터가 있는 단어 벡터의 합을 구한다.\n",
    "# 마지막으로 사용한 단어의 전체 개수로 나눔으로 평균 벡터의 값을 구한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 리뷰에 대해 각 리뷰의 평균 벡터를 구하는 함수\n",
    "def get_dataset(reviews, model, num_features):\n",
    "    dataset = list()\n",
    "\n",
    "    for s in reviews:\n",
    "        dataset.append(get_features(s, model, num_features))\n",
    "\n",
    "    reviewFeatureVecs = np.stack(dataset)\n",
    "    \n",
    "    return reviewFeatureVecs\n",
    "\n",
    "# get_dataset(reviews: 학습 데이터, model: word2vec 모델, num_features: word2vec으로 임베딩할 때 정한 벡터의 차원 수)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\python36\\venv\\kmu\\lib\\site-packages\\ipykernel_launcher.py:11: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "test_data_vecs = get_dataset(sentences, model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_vecs.shape # np.stack를 사용하여 2차원의 배열을 만듬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.05427849, -0.30749243,  0.04156079, ..., -0.06850925,\n",
       "         0.12413732,  0.05045112],\n",
       "       [-0.15822639, -0.19824882,  0.03656638, ..., -0.28203326,\n",
       "        -0.08932111,  0.08399303],\n",
       "       [-0.02151882, -0.15708731,  0.05830048, ...,  0.00422297,\n",
       "         0.11921615, -0.05602044],\n",
       "       ...,\n",
       "       [-0.0120516 , -0.14159724, -0.0530271 , ..., -0.02531817,\n",
       "         0.11852324,  0.27780178],\n",
       "       [-0.07804546, -0.30542907, -0.00167563, ..., -0.18056087,\n",
       "         0.11587103,  0.15989284],\n",
       "       [-0.04820551, -0.4560491 , -0.13065109, ...,  0.0555407 ,\n",
       "         0.09740985,  0.19358613]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습, 검증 데이터셋 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "X = test_data_vecs\n",
    "y = np.array(sentiments)\n",
    "\n",
    "X_train, X_eval, y_train, y_eval = train_test_split(X, y, test_size=TEST_SPLIT, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\python36\\venv\\kmu\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='warn', n_jobs=None, penalty='l2', random_state=None,\n",
       "          solver='warn', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lgs = LogisticRegression(class_weight='balanced')\n",
    "lgs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = lgs.predict(X_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------\n",
      "Accuracy: 0.865600\n",
      "Precision: 0.860883\n",
      "Recall: 0.874553\n",
      "F1-Score: 0.867664\n",
      "AUC: 0.934759\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test, (lgs.predict_proba(X_eval)[:, 1]))\n",
    "auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "print(\"------------\")\n",
    "print(\"Accuracy: %f\" % lgs.score(X_eval, y_eval))  #checking the accuracy\n",
    "print(\"Precision: %f\" % metrics.precision_score(y_eval, predicted))\n",
    "print(\"Recall: %f\" % metrics.recall_score(y_eval, predicted))\n",
    "print(\"F1-Score: %f\" % metrics.f1_score(y_eval, predicted))\n",
    "print(\"AUC: %f\" % auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 테스트 데이터 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_CLEAN_DATA = 'test_clean.csv'\n",
    "\n",
    "test_data = pd.read_csv(DATA_IN_PATH + TEST_CLEAN_DATA)\n",
    "\n",
    "test_review = list(test_data['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = list()\n",
    "for review in test_review:\n",
    "    test_sentences.append(review.split())\n",
    "# 하나의 문자열을 각 단어의 리스트로 만들기."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\python36\\venv\\kmu\\lib\\site-packages\\ipykernel_launcher.py:11: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "test_data_vecs = get_dataset(test_sentences, model, num_features)\n",
    "# 학습 데이터를 학습시킨 모델을 사용하여 각 리뷰에 대한 특징값을 만든다. (model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predicted = lgs.predict(test_data_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = list(test_data['id'])\n",
    "\n",
    "answer_dataset = pd.DataFrame({'id': ids, 'sentiment': test_predicted})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(DATA_OUT_PATH):\n",
    "    os.makedirs(DATA_OUT_PATH)\n",
    "\n",
    "answer_dataset.to_csv(DATA_OUT_PATH + 'lgs_w2v_answer.csv', index=False, quoting=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 캐글에 데이터 제출시 결과 점수 : 0.85920\n",
    "### public 점수로 비교해본 결과 295등 정도(private 점수는 공개되지 않아 비교 불가)\n",
    "### TF-IDF를 사용했을 때(0.85368, 314등 정도)보다 약간 오름"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
